
<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <title>
        DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion
    </title>
    <meta content="GLAMR" property="og:title" />
    <meta content="A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID) for object association. This pipeline is partially motivated by recent progress in both object detec- tion and re-ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin- guishing appearance and re-ID models are sufficient for es- tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have similar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it “DanceTrack”. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks." name="description" property="og:description" />
    <meta content="https://github.com/DanceTrack" property="og:url" />
    <meta name="keywords" content="Multi-Object Tracking in Uniform Appearance and Diverse Motion">

    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script defer src="js/fontawesome.all.min.js"></script>
</head>

<body>
    <div class="n-header">
    </div>
    <div class="n-title">
        <h1>
            DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion
        </h1>
    </div>
    <div class="n-byline">
        <div class="byline">
            <ul class="authors">
                <li>
                    <a href="https://peizesun.github.io/" target="_blank">Peize Sun*</a><sup>1</sup>
                </li>
                <li>
                    <a href="http://www.jinkuncao.com/" target="_blank">Jinkun Cao*</a><sup>2</sup>
                </li>
                <li>
                    <a href="" target="_blank">Yi Jiang</a><sup>3</sup>
                </li>
                <li>
                    <a href="https://shallowyuan.github.io/" target="_blank">Zehuan Yuan</a><sup>3</sup>
                </li>
                <li>
                    <a href="https://songbai.site/" target="_blank">Song Bai</a><sup>3</sup>
                </li>                
                <li>
                    <a href="http://www.cs.cmu.edu/~kkitani/" target="_blank">Kris Kitani</a><sup>2</sup>
                </li>
                <li>
                    <a href="http://luoping.me/" target="_blank">Ping Luo</a><sup>1</sup>
                </li>
            </ul>
            <ul class="authors affiliations">
                <li>
                    <sup>
                        1
                    </sup>
                    The University of Hong Kong
                </li>
                <li>
                    <sup>
                        2
                    </sup>
                    Carnegie Mellon University
                </li>
                <li>
                    <sup>
                        3
                    </sup>
                    ByteDance Inc.
                </li>
            </ul>
            <ul class="authors affiliations">
                <li>
                    <sup>
                        *
                    </sup>
                    equal contribution
                </li> 
            </ul>
<!--             <ul class="authors venue">
                <li>
                    : 
                </li>
            </ul> -->
            <ul class="authors links">
                <li>
                    <a href="https://arxiv.org/abs/2111.14690" target="_blank">
                        <button class="btn"><i class="fa fa-file-pdf"></i> Paper</button>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/DanceTrack" target="_blank">
                        <button class="btn"><i class="fab fa-github"></i> Code</button>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/DanceTrack" target="_blank">
                        <button class="btn"><i class="fab fa-github"></i> Dataset</button>
                    </a>
                </li> 
                <li>
                    <a href="https://competitions.codalab.org/competitions/35786" target="_blank">
                        <button class="btn"><i class="fab fa-youtube fa-w-18"></i> CodaLab</button>
                    </a>
                </li>
            </ul>
        </div>
    </div>

<!-- <iframe width="760" height="381" src="https://www.youtube.com/embed/IvxeJRg4rYg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    
    <div class="n-article">
        <div class="n-page video">
<!--             <video class="centered shadow" width="100%" autoplay muted loop playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
<!--                 <source src="https://www.youtube.com/embed/IvxeJRg4rYg" type="video/mp4" /> -->
<!--             </video>  -->
<!--         <h2>
            Narrated Results Video
        </h2> --> 
        <div class="videoWrapper shadow">
            <iframe width="705" height="397" border-style=none src="https://www.youtube.com/embed/IvxeJRg4rYg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>            
            
            <div class="videocaption" style="margin-bottom: 1rem">
                <div>
                    DanceTrack is a multi-human tracking dataset with two emphasized properties, (1) <span class="emph">uniform appearance</span>: humans are in highly similar and almost undistinguished appearance, (2) <span class="emph">diverse motion</span>: humans are in complicated motion pattern and their relative positions exchange frequently. We expect the combination of uniform appearance and complicated motion pattern makes DanceTrack a platform to encourage more comprehensive and intelligent multi-object tracking algorithms.
                </div>   
            </div>
        </div>

        <h2 id="abstract">
            Abstract
        </h2>
        <p>
            A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID) for object association. This pipeline is partially motivated by recent progress in both object detec- tion and re-ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin- guishing appearance and re-ID models are sufficient for es- tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have similar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it “DanceTrack”. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks.
        </p>
        
        <h2>
            Dataset
        </h2>
        <img class="figure" src="media/dancetrack.jpg" alt="Dataset Overview">
        
<!--         <h2 id="results">
            Results
        </h2>
        <h3 class="results" id="sample">
            Generative Motion Infilling with Multiple Samples
        </h3>
        <video class="centered shadow" width="100%" autoplay muted loop playsinline>
            <source src="assets/media/glamr_sample.mp4#t=0.001" type="video/mp4" />
        </video>
        <div class="videocaption">
            <div>GLAMR uses generative motion infiller to infill multiple plausible motions for invisible people.</div>
        </div>

        <h3 class="results">
            3DPW Sequences
        </h3>
        <video class="centered shadow video_without_cap" width="100%" autoplay muted loop playsinline>
            <source src="assets/media/glamr_res1.mp4#t=0.001" type="video/mp4" />
        </video>
        <video class="centered shadow video_without_cap" width="100%" autoplay muted loop playsinline>
            <source src="assets/media/glamr_res2.mp4#t=0.001" type="video/mp4" />
        </video>
        <video class="centered shadow video_without_cap" width="100%" autoplay muted loop playsinline>
            <source src="assets/media/glamr_res3.mp4#t=0.001" type="video/mp4" />
        </video> -->
        
        
<!--         <h2>
            Narrated Results Video
        </h2>
        <div class="videoWrapper shadow">
            <iframe width="705" height="397" border-style=none src="https://www.youtube.com/embed/wpObDXcYueo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>

        <h2>
            Overview
        </h2>
        <img class="figure" src="assets/media/glamr_overview.png" alt="GLAMR Overview"> -->

        <h2 id="citation">
            Citation
        </h2>
        <pre class="bibtex">
            <code>
@inproceedings{sun2022dance,
    title={DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion},
    author={Sun, Peize and Cao, Jinkun and Jiang, Yi and Yuan, Zehuan and Bai, Song and Kitani, Kris and Luo, Ping},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2022}
}
            </code>
        </pre>
        
        <br>
        <h3>
        Acknowledgement
        </h3>    
        <p>
            We would like to thank the annotator teams and coordinators. We also like to thank Xinshuo Weng, Yifu Zhang for valuable discussion and suggestions, Vivek Roy, Pedro Morgado, Shuyang Sun for proof reading. This website is developed referring to <a href="https://nvlabs.github.io/GLAMR/" target="_blank">GLAMR</a>.
        </p>
        
    </div>
</body>

</html>
    
                   
