<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <title>
        DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion
    </title>
    <meta content="GLAMR" property="og:title" />
    <meta content="A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID) for object association. This pipeline is partially motivated by recent progress in both object detec- tion and re-ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin- guishing appearance and re-ID models are sufficient for es- tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have similar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it “DanceTrack”. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks." name="description" property="og:description" />
    <meta content="https://github.com/DanceTrack" property="og:url" />
    <meta name="keywords" content="Multi-Object Tracking in Uniform Appearance and Diverse Motion">

    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script defer src="js/fontawesome.all.min.js"></script>
</head>

<body>
    <div class="n-header">
    </div>
    <div class="n-title">
        <h1>
            DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion
        </h1>
    </div>
    <div class="n-byline">
        <div class="byline">
            <ul class="authors">
                <li>
                    <a href="https://peizesun.github.io/" target="_blank">Peize Sun*</a><sup>1</sup>
                </li>
                <li>
                    <a href="http://www.jinkuncao.com/" target="_blank">Jinkun Cao*</a><sup>2</sup>
                </li>
                <li>
                    <a href="" target="_blank">Yi Jiang</a><sup>3</sup>
                </li>
                <li>
                    <a href="https://shallowyuan.github.io/" target="_blank">Zehuan Yuan</a><sup>3</sup>
                </li>
                <li>
                    <a href="https://songbai.site/" target="_blank">Song Bai</a><sup>3</sup>
                </li>                
                <li>
                    <a href="http://www.cs.cmu.edu/~kkitani/" target="_blank">Kris Kitani</a><sup>2</sup>
                </li>
                <li>
                    <a href="http://luoping.me/" target="_blank">Ping Luo</a><sup>1</sup>
                </li>
            </ul>
            <ul class="authors affiliations">
                <li>
                    <sup>
                        1
                    </sup>
                    The University of Hong Kong
                </li>
                <li>
                    <sup>
                        2
                    </sup>
                    Carnegie Mellon University
                </li>
                <li>
                    <sup>
                        3
                    </sup>
                    ByteDance Inc.
                </li>
            </ul>
            <ul class="authors affiliations">
                <li>
                    <sup>
                        *
                    </sup>
                    equal contribution
                </li> 
            </ul>
<!--             <ul class="authors venue">
                <li>
                    : 
                </li>
            </ul> -->
            <ul class="authors links">
                <li>
                    <a href="https://arxiv.org/abs/2111.14690" target="_blank">
                        <button class="btn"><i class="fa fa-file-pdf"></i> Paper</button>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/DanceTrack" target="_blank">
                        <button class="btn"><i class="fab fa-github"></i> Code</button>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/DanceTrack" target="_blank">
                        <button class="btn"><i class="fab fa-github"></i> Dataset</button>
                    </a>
                </li> 
                <li>
                    <a href="https://competitions.codalab.org/competitions/35786" target="_blank">
                        <button class="btn"><i class="fab fa-youtube fa-w-18"></i> CodaLab</button>
                    </a>
                </li>
            </ul>
        </div>
    </div>

<!-- <iframe width="760" height="381" src="https://www.youtube.com/embed/IvxeJRg4rYg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    
    <div class="n-article">
        <div class="n-page video">
<!--             <video class="centered shadow" width="100%" autoplay muted loop playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
<!--                 <source src="https://www.youtube.com/embed/IvxeJRg4rYg" type="video/mp4" /> -->
<!--             </video>  -->
<!--         <h2>
            Narrated Results Video
        </h2> --> 
        <div class="videoWrapper shadow">
            <iframe width="705" height="397" border-style=none src="https://www.youtube.com/embed/IvxeJRg4rYg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>            
            
            <div class="videocaption" style="margin-bottom: 1rem">
                <div>
                    DanceTrack is a multi-human tracking dataset with two emphasized properties, (1) <span class="emph">uniform appearance</span>: humans are in highly similar and almost undistinguished appearance, (2) <span class="emph">diverse motion</span>: humans are in complicated motion pattern and their relative positions exchange frequently. We expect the combination of uniform appearance and complicated motion pattern makes DanceTrack a platform to encourage more comprehensive and intelligent multi-object tracking algorithms.
                </div>   
            </div>
        </div>

        <h2 id="abstract">
            Abstract
        </h2>
        <p>
            A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID) for object association. This pipeline is partially motivated by recent progress in both object detec- tion and re-ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin- guishing appearance and re-ID models are sufficient for es- tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have similar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it “DanceTrack”. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks.
        </p>
        
        <h2>
            Dataset
        </h2>
        <div>
            <img class="figure" src="media/dancetrack.jpg" width="100%" alt="Dataset Overview">
        </div>
        <p>
        Scene samples from DanceTrack. (a) outdoor scene, (b) low-lighting scene, (c) large group of dancing people; (d) gymnastics scene where the motion is usually even more diverse and people have more aggressive deformation. 
        </p>
        <p>
        DanceTrack consists of:
            <ul>
              <li>100 videos of group dance, 40 training videos, 25 validation videos and 35 test videos</li>
              <li>990 unique instances with average length of 52.9s</li>
              <li>105k frames and 877k high-quality bounding boxes by 20fps annotation</li>
            </ul>
        </p>
    
         <h2 id="results">
            Results
        </h2>
        
        <h3 class="results" id="oracle">
            Oracle Analysis
        </h3>
        <div>
            <img class="figure" src="media/oracle.jpg" width="100%" alt="Oracle Analysis">
        </div>
        <p>
        Oracle analysis of different association models on MOT17 and DanceTrack validation set, where the detection boxes are ground-truth boxes. The result shows the evident increased difficulty of performing multi-object tracking on DanceTrack than MOT17 dataset.
        </p>
        
        <h3 class="results" id="benchmark">
            Benchmark Result
        </h3>
        <div>
            <img class="figure" src="media/benchmark.jpg" width="100%" alt="Benchmark Result">
        </div>
        <p>
        Benchmark results of investigated algorithms on MOT17 and DanceTrack test set. DanceTrack makes detection easier (higher MOTA and DetA scoers) but still brings significant tracking performance drop compared to MOT17 (lower HOTA, AssA and IDF1 scores). This result reveals the bottleneck of multi-object tracking on DanceTrack is on the association part.
        </p>

        <h3 class="results" id="benchmark">
            Association Strategy
        </h3>
        <div>
            <img class="figure" src="media/motion.jpg" width="100%" alt="Benchmark Result">
        </div>
        <p>
        Comparisons of different association strategies on DanceTrack validation set. The detection results are output by the same YOLOX detector. Both Kalman filter and LSTM outperform naive IoU association by a large margin, indicating the great potential of motion models in tracking objects, especially when appearance cues are not reliable. We expect to see more researches in this field.
        </p>

        <h2 id="citation">
            Citation
        </h2>
        <pre class="bibtex">
            <code>
@inproceedings{sun2022dance,
    title={DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion},
    author={Sun, Peize and Cao, Jinkun and Jiang, Yi and Yuan, Zehuan and Bai, Song and Kitani, Kris and Luo, Ping},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2022}
}
            </code>
        </pre>
        
        <br>
        <h3>
        Acknowledgement
        </h3>    
        <p>
            We would like to thank the annotator teams and coordinators. We also like to thank Xinshuo Weng, Yifu Zhang for valuable discussion and suggestions, Vivek Roy, Pedro Morgado, Shuyang Sun for proof reading. This website is developed referring to <a href="https://nvlabs.github.io/GLAMR/" target="_blank">GLAMR</a>.
        </p>
        
    </div>
</body>

</html>
    
                   
